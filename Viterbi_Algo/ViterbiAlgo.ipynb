{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNTQYn26vxjITZUt4VZzh0K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K7tpBjXldAHT","executionInfo":{"status":"ok","timestamp":1707290709771,"user_tz":-330,"elapsed":9558,"user":{"displayName":"Sayak Ghorai","userId":"05820323559307504147"}},"outputId":"171a102c-bcb4-44e0-d980-a0cb38e75a37"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"]}],"source":["!pip install nltk"]},{"cell_type":"code","source":["import nltk\n","nltk.download('all')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c7KUPrI8dFbJ","executionInfo":{"status":"ok","timestamp":1707386903597,"user_tz":-330,"elapsed":45011,"user":{"displayName":"Sayak Ghorai","userId":"05820323559307504147"}},"outputId":"a6139621-4fa7-43d5-cf31-4bb5b47da1cd"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/abc.zip.\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/alpino.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n","[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown.zip.\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/chat80.zip.\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/city_database.zip.\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2000.zip.\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2002.zip.\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/crubadan.zip.\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dolch.zip.\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n","[nltk_data]    | Downloading package extended_omw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/floresta.zip.\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ieer.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/indian.zip.\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/kimmo.zip.\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/moses_sample.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/paradigms.zip.\n","[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pe08.zip.\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/perluniprops.zip.\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pil.zip.\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pl196x.zip.\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ppattach.zip.\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ptb.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/qc.zip.\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/rslp.zip.\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/rte.zip.\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/senseval.zip.\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/smultron.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/state_union.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/swadesh.zip.\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/switchboard.zip.\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Unzipping help/tagsets.zip.\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/timit.zip.\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/toolbox.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr.zip.\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr2.zip.\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet.zip.\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/webtext.zip.\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n","[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ycoe.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["from nltk.corpus import brown\n","\n","# Training the HMM tagger\n","brown_tagged_sents = brown.tagged_sents(categories='news')\n","size = int(len(brown_tagged_sents) * 0.9)\n","train_sents = brown_tagged_sents[:size]\n","test_sents = brown_tagged_sents[size:]\n","\n","# Defining the HMM tagger\n","hmm_tagger = nltk.HiddenMarkovModelTagger.train(train_sents)\n","\n","# Function to implement Viterbi algorithm\n","def viterbi(words):\n","    return hmm_tagger.tag(words)"],"metadata":{"id":"bYmn9dQNd5c5","executionInfo":{"status":"ok","timestamp":1707387168194,"user_tz":-330,"elapsed":1284,"user":{"displayName":"Sayak Ghorai","userId":"05820323559307504147"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Test the Viterbi algorithm\n","test_sent = \"This is John. One day he saw a saw and decided to take it to the dinning table\".split()\n","res = viterbi(test_sent)\n","for ele in res:\n","  print(ele)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TZV6U9LCjL0y","executionInfo":{"status":"ok","timestamp":1707387178003,"user_tz":-330,"elapsed":4325,"user":{"displayName":"Sayak Ghorai","userId":"05820323559307504147"}},"outputId":"7c4b6c9d-1089-4623-d1e6-6eb333b1ce0c"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["('This', 'DT')\n","('is', 'BEZ')\n","('John.', 'RB')\n","('One', 'CD')\n","('day', 'NN')\n","('he', 'PPS')\n","('saw', 'VBD')\n","('a', 'AT')\n","('saw', 'NN')\n","('and', 'CC')\n","('decided', 'VBD')\n","('to', 'TO')\n","('take', 'VB')\n","('it', 'PPO')\n","('to', 'IN')\n","('the', 'AT')\n","('dinning', 'JJ')\n","('table', 'NN')\n"]}]},{"cell_type":"code","source":["from collections import defaultdict\n","\n","# Define transition probabilities based on observed tags\n","transition_probabilities = {\n","    'DT': {'DT': 0, 'NN': 0.5, 'JJ': 0.5},\n","    'NN': {'DT': 0.2, 'NN': 0.2, 'VB': 0.6},\n","    'JJ': {'NN': 1.0},\n","    'RB': {'CD': 1.0},\n","    'CD': {'NN': 1.0},\n","    'PPS': {'VBD': 1.0},\n","    'VBD': {'AT': 0.5, 'JJ': 0.5},\n","    'AT': {'NN': 1.0},\n","    'CC': {'VBD': 1.0},\n","    'TO': {'VB': 1.0},\n","    'VB': {'PPO': 1.0},\n","    'PPO': {'IN': 1.0},\n","    'IN': {'AT': 1.0},\n","}\n","\n","# Define starting probability\n","start_probabilities = {'DT': 1.0}\n","\n","# Function to find the most probable sequence\n","def find_probable_sequence(tagged_words, transition_probabilities, start_probabilities):\n","    sequence = []\n","    current_tag = None\n","\n","    # Initialize the current_tag based on starting probabilities\n","    max_start_prob = 0\n","    for tag, prob in start_probabilities.items():\n","        if prob > max_start_prob:\n","            max_start_prob = prob\n","            current_tag = tag\n","\n","    sequence.append((tagged_words[0][0], current_tag))\n","\n","    # Iterate over the remaining words\n","    for i in range(1, len(tagged_words)):\n","        prev_tag = current_tag\n","        current_word, _ = tagged_words[i]\n","\n","        max_prob = 0\n","        for next_tag, prob in transition_probabilities[current_tag].items():\n","            if prob > max_prob:\n","                max_prob = prob\n","                current_tag = next_tag\n","\n","        sequence.append((current_word, current_tag))\n","\n","    return sequence\n","\n","# Example tagged words\n","tagged_words = [('This', 'DT'), ('John.', 'RB'), ('One', 'CD'), ('is', 'BEZ'),('day', 'NN'), ('he', 'PPS'), ('saw', 'VBD'), ('a', 'AT'), ('saw', 'NN'), ('and', 'CC'), ('decided', 'VBD'), ('to', 'TO'), ('take', 'VB'), ('it', 'PPO'), ('to', 'IN'), ('the', 'AT'), ('dinning', 'JJ'), ('table', 'NN')]\n","\n","# Find the most probable sequence\n","probable_sequence = find_probable_sequence(tagged_words, transition_probabilities, start_probabilities)\n","\n","# Print the most probable sequence\n","for word, tag in probable_sequence:\n","    print(f'({word}, {tag})', end=' ')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rdh2d_nTNrm5","executionInfo":{"status":"ok","timestamp":1707387491927,"user_tz":-330,"elapsed":479,"user":{"displayName":"Sayak Ghorai","userId":"05820323559307504147"}},"outputId":"f4aa9430-c2c7-423a-f97d-bf612a085dff"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["(This, DT) (John., NN) (One, VB) (is, PPO) (day, IN) (he, AT) (saw, NN) (a, VB) (saw, PPO) (and, IN) (decided, AT) (to, NN) (take, VB) (it, PPO) (to, IN) (the, AT) (dinning, NN) (table, VB) "]}]},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import brown\n","from nltk.tag import HiddenMarkovModelTagger\n","\n","# Training the HMM tagger\n","brown_tagged_sents = brown.tagged_sents(categories='news')\n","size = int(len(brown_tagged_sents) * 0.9)\n","train_sents = brown_tagged_sents[:size]\n","test_sents = brown_tagged_sents[size:]\n","\n","# Defining the HMM tagger\n","hmm_tagger = HiddenMarkovModelTagger.train(train_sents)\n","\n","# Function to implement Viterbi algorithm and get probabilities\n","def viterbi_with_prob(words):\n","    tagged_words = hmm_tagger.tag(words)\n","    probabilities = [prob for _, prob in tagged_words]\n","    return tagged_words, probabilities\n","\n","# Example words\n","words_to_label = \"This is John. One day he saw a saw and decided to take it to the dining table\".split()\n","\n","# Use Viterbi algorithm to label words and get probabilities\n","tagged_words, probabilities = viterbi_with_prob(words_to_label)\n","\n","# Print the labeled sequence with associated probabilities\n","for (word, _), prob in zip(tagged_words, probabilities):\n","    print(f'({word}, {prob})', end=' ')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_mr70gCoO8q6","executionInfo":{"status":"ok","timestamp":1707388217778,"user_tz":-330,"elapsed":6346,"user":{"displayName":"Sayak Ghorai","userId":"05820323559307504147"}},"outputId":"41c7993d-a55f-4215-9a96-495948a13f49"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["(This, DT) (is, BEZ) (John., RB) (One, CD) (day, NN) (he, PPS) (saw, VBD) (a, AT) (saw, NN) (and, CC) (decided, VBD) (to, TO) (take, VB) (it, PPO) (to, IN) (the, AT) (dining, NN) (table, NN) "]}]},{"cell_type":"code","source":["import nltk\n","\n","# Download the Penn Treebank corpus\n","nltk.download('treebank')\n","\n","# Load the Penn Treebank corpus\n","corpus = nltk.corpus.treebank.tagged_sents()\n","\n","# Count transition probabilities from the corpus\n","transition_counts = {}\n","pos_counts = {}\n","for sentence in corpus:\n","    previous_tag = None\n","    for word, tag in sentence:\n","        if previous_tag is not None:\n","            if previous_tag not in transition_counts:\n","                transition_counts[previous_tag] = {}\n","            if tag not in transition_counts[previous_tag]:\n","                transition_counts[previous_tag][tag] = 0\n","            transition_counts[previous_tag][tag] += 1\n","        if tag not in pos_counts:\n","            pos_counts[tag] = 0\n","        pos_counts[tag] += 1\n","        previous_tag = tag\n","\n","# Calculate transition probabilities\n","transition_probabilities = {}\n","for prev_tag, next_tags in transition_counts.items():\n","    transition_probabilities[prev_tag] = {}\n","    total_transitions = sum(next_tags.values())\n","    for next_tag, count in next_tags.items():\n","        transition_probabilities[prev_tag][next_tag] = count / total_transitions\n","\n","# Define a simple POS tagger\n","def simple_pos_tagger(word):\n","    if word.lower() in ['the', 'a', 'an']:\n","        return 'DT'  # Determiner\n","    else:\n","        return 'NN'  # Noun (just for simplicity)\n","\n","# Implement Viterbi algorithm\n","def viterbi(tokens, initial_probabilities, transition_probabilities, pos_tagger):\n","    # Initialize viterbi matrix\n","    viterbi_matrix = [{}]\n","\n","    # Initialize backpointer\n","    backpointer = [{}]\n","\n","    # Initialize first column of viterbi matrix\n","    for pos_tag in initial_probabilities:\n","        viterbi_matrix[0][pos_tag] = initial_probabilities[pos_tag] * transition_probabilities[pos_tag].get(pos_tagger(tokens[0]), 0)\n","        backpointer[0][pos_tag] = None\n","\n","    # Fill in the viterbi matrix\n","    for t in range(1, len(tokens)):\n","        viterbi_matrix.append({})\n","        backpointer.append({})\n","        for pos_tag in initial_probabilities:\n","            max_prob = max(viterbi_matrix[t-1][prev_tag] * transition_probabilities[prev_tag].get(pos_tag, 0) for prev_tag in initial_probabilities)\n","            viterbi_matrix[t][pos_tag] = max_prob * transition_probabilities[pos_tag].get(pos_tagger(tokens[t]), 0)\n","            backpointer[t][pos_tag] = max(initial_probabilities.keys(), key=lambda prev_tag: viterbi_matrix[t-1][prev_tag] * transition_probabilities[prev_tag].get(pos_tag, 0))\n","\n","    # Find the tag sequence with highest probability\n","    max_prob = max(viterbi_matrix[-1][pos_tag] for pos_tag in initial_probabilities)\n","    tag_sequence = []\n","    prev_tag = max(initial_probabilities.keys(), key=lambda pos_tag: viterbi_matrix[-1][pos_tag])\n","    for t in range(len(tokens)-1, -1, -1):\n","        tag_sequence.insert(0, prev_tag)\n","        prev_tag = backpointer[t][prev_tag]\n","\n","    return tag_sequence\n","\n","# Run Viterbi algorithm on an example sentence\n","sentence = \"The quick brown fox jumps over the lazy dog\"\n","tokens = nltk.word_tokenize(sentence)\n","initial_probabilities = {'DT': 0.5, 'NN': 0.5}  # Initial probabilities can be refined too\n","predicted_tags = viterbi(tokens, initial_probabilities, transition_probabilities, simple_pos_tagger)\n","\n","# Print the result\n","print(\"Predicted POS tags:\", predicted_tags)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b2y2Sz9pTP3X","executionInfo":{"status":"ok","timestamp":1707388971089,"user_tz":-330,"elapsed":1299,"user":{"displayName":"Sayak Ghorai","userId":"05820323559307504147"}},"outputId":"5a77bcd6-2faa-4f03-87ea-46590ca55463"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package treebank to /root/nltk_data...\n","[nltk_data]   Package treebank is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Predicted POS tags: ['NN', 'NN', 'NN', 'NN', 'NN', 'NN', 'NN', 'NN', 'NN']\n"]}]}]}